### fitting the data to kNN ###
K = 2
ko = knn(x.train, x.test, y.train, K)
knn.pred = as.numeric(ko == "Up")
knn.pred
tb.knn = table(ko, y.test)
tb.knn
### ROC curve for kNN ###
knn.probs = attributes(knn(x.train, x.test, y.train, K, prob=TRUE))$prob
new.knn.probs = 1 - knn.probs
knn.probs.final = ifelse(knn.pred == 1, knn.probs, new.knn.probs)
roc.knn = roc(y.test ,knn.probs.final)$auc
roc.knn
#roc.knn = roc(response=(y.test), predictor=new.knn.probs)
#roc.knn$auc
plot(roc.knn, add = TRUE, col = "orange")
roc.knn.plot = roc(response=(y.test), predictor=knn.probs.final)
plot(roc.knn, add = TRUE, col = "orange")
rm(list=ls())
library(ISLR)
library(class)
library(pROC)
library(randomForest)
set.seed(4061)
### loading the data 3(a)###
x = Smarket[,-9]
y = as.factor(Smarket$Direction)
train = sample(1:nrow(Smarket),1000)
data_rf = data.frame(x, y)
### Splitting data into train and test sets ###
train_data = data_rf[train,]
test_data = data_rf[-train,]
### Fitting Random Forest to the training data ###
rf.out = randomForest(y~., train_data)
rf.yhat.train = predict(rf.out, train_data, type="class")
tb.rf.train = table(rf.yhat.train, train_data$y)
tb.rf.train
### Performing predictions on the test data 3(b)###
rf.yhat.test = predict(rf.out, test_data, type = "class")
tb.rf.test = table(rf.yhat.test, test_data$y)
tb.rf.test
### Plot ROC Curve for the test data ###
rf.test.probs = predict(rf.out, test_data, type="prob")
roc.rf = roc(response=(test_data$y), predictor=rf.test.probs[,1])
roc.rf$auc
plot(roc.rf)
### Preparing data for kNN model 3(c)###
x.train = x[train,]
x.test = x[-train,]
y.train = y[train]
y.test = y[-train]
### fitting the data to kNN ###
K = 2
ko = knn(x.train, x.test, y.train, K)
knn.pred = as.numeric(ko == "Up")
tb.knn = table(ko, y.test)
tb.knn
### ROC curve for kNN ###
knn.probs = attributes(knn(x.train, x.test, y.train, K, prob=TRUE))$prob
new.knn.probs = 1 - knn.probs
knn.probs.final = ifelse(knn.pred == 1, knn.probs, new.knn.probs)
roc.knn = roc(y.test ,knn.probs.final)$auc
roc.knn$auc
rm(list=ls())
rm(list=ls())
library(ISLR)
library(class)
library(pROC)
library(randomForest)
set.seed(4061)
### loading the data 3(a)###
x = Smarket[,-9]
y = as.factor(Smarket$Direction)
train = sample(1:nrow(Smarket),1000)
data_rf = data.frame(x, y)
### Splitting data into train and test sets ###
train_data = data_rf[train,]
test_data = data_rf[-train,]
### Fitting Random Forest to the training data ###
rf.out = randomForest(y~., train_data)
rf.yhat.train = predict(rf.out, train_data, type="class")
tb.rf.train = table(rf.yhat.train, train_data$y)
tb.rf.train
### Performing predictions on the test data 3(b)###
rf.yhat.test = predict(rf.out, test_data, type = "class")
tb.rf.test = table(rf.yhat.test, test_data$y)
tb.rf.test
### Plot ROC Curve for the test data ###
rf.test.probs = predict(rf.out, test_data, type="prob")
roc.rf = roc(response=(test_data$y), predictor=rf.test.probs[,1])
roc.rf$auc
plot(roc.rf)
### Preparing data for kNN model 3(c)###
x.train = x[train,]
x.test = x[-train,]
y.train = y[train]
y.test = y[-train]
### fitting the data to kNN ###
K = 2
ko = knn(x.train, x.test, y.train, K)
knn.pred = as.numeric(ko == "Up")
tb.knn = table(ko, y.test)
tb.knn
### ROC curve for kNN ###
knn.probs = attributes(knn(x.train, x.test, y.train, K, prob=TRUE))$prob
new.knn.probs = 1 - knn.probs
knn.probs.final = ifelse(knn.pred == 1, knn.probs, new.knn.probs)
roc.knn = roc(y.test ,knn.probs.final)
roc.knn$auc
plot(roc.knn, add = TRUE, col = "orange")
legend("bottomright",legend = c("Random Forest", "KNN"), col = c("black", "orange"), lwd = 5)
### KNN classification for different k values 3(d)###
set.seed(4061)
M = 1000
train = sample(1:nrow(Smarket), M)
x.train = x[train,]
x.test = x[-train,]
y.train = y[train]
y.test = y[-train]
Kmax = 10
acc = numeric(Kmax)
for(k in 1:Kmax){
ko = knn(x.train, x.test, y.train, k)
tb = table(ko, y.test)
acc[k] = sum(diag(tb)) / sum(tb)
}
misclass.err = 1 - acc
plot(misclass.err, pch=20, t='b', main = "Plot of K vs Misclassification Error Rate",
ylab = "Misclassification Error Rate", xlab="K")
?roc
rm(list=ls())
library(ISLR)
df = Carseats
View(df)
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
CS = data.frame(df, High)
View(CS)
set.seed(4061)
?sample
n = length(df)
idx = sample(1:n, 350)
# simulate data:
set.seed(1)
n = 100
mark = rnorm(n, m=50, sd=10)
choc = rnorm(n, m=60, sd=5)
summary(mark)
summary(choc)
int = 10
a = 2
b = 4
# split the data into train+test (50%-50%):
x = data.frame(mark,choc)
i.train = sample(1:n, 50)
rm(list=ls())
library(ISLR)
df = Carseats
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
CS = data.frame(df, High)
set.seed(4061)
n = length(df)
idx = sample(1:n, 350)
idx = sample(1:n, 350, replace = TRUE)
X.train = CS[idx,]
X.test = CS[-idx,]
y.train = CS[idx]
y.test = CS[-idx]
View(y.train)
rm(list=ls())
library(ISLR)
df = Carseats
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
CS = data.frame(df, High)
set.seed(4061)
n = length(df)
idx = sample(1:n, 350)
rm(list=ls())
library(ISLR)
df = Carseats
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
CS = data.frame(df, High)
set.seed(4061)
idx = sample(1:length(df), 350)
X.train = CS[idx,]
X.test = CS[-idx,]
y.train = CS[idx]
y.test = CS[-idx]
n = nrow(df)
idx = sample(1:n , 350)
X.train = CS[idx,]
X.test = CS[-idx,]
y.train = CS[idx]
y.train = CS[idx]
y.test = CS[-idx]
n = nrow(df)
idx = sample(1:n , 350)
X.train <- CS[idx, ]
X.test <- CS[-idx, ]
y.train <- CS$High[idx]
y.test <- CS$High[-idx]
install.packages("e1071")
install.packages("e1071")
#install.packages("e1071")
library(e1071)
svmo = svm(X.train, y.train, kernel='polynomial')
?svm
rm(list=ls())
rm(list=ls())
library(ISLR)
library(e1071)
df = Carseats
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
View(df)
CS = data.frame(df, High)
View(CS)
CS$Sales = NULL
X = CS
y = CS$High
X$High = NULL
set.seed(4061)
n = nrow(X)
idx = sample(1:n , 350)
X.train = X[idx, ]
X.test = X[-idx]
y.train = y[idx, ]
y.train = y[idx]
y.test = y[-idx]
X.test = X[-idx, ]
### Fitting SVM model (from e1071 package)
svmo = svm(X.train, y.train, kernel='polynomial')
View(X.train)
### Error occuring here is because the covariates cannot be supported
### in factor category, so perform one hot encoding to convert them.
new_data = model.matrix(y~.+0, data = X)
View(new_data)
new.X.train = new_data[idx, ]
new.X.test = new_data[-idx, ]
y.train = as.factor(y.train)
### Now fitting the SVM model
svm.p = svm(new.X.train, y.train, kernel = 'polynomial')
summary(svm.p)
svm.l = svm(new.X.train, y.train, kernel = 'linear')
summary(svm.l)
plot(svm.p)
svm.p.train_preds = fitted(svm.p)
svm.l.train_preds = fitted(svm.l)
table(y.train, svm.p.train_preds)
table(y.train, svm.l.train_preds)
### Evaluating model on test data
svm.p.test_preds = predict(svm.p, newdata = new.X.test)
y.test = as.factor(y.test)
svm.l.test_preds = predict(svm.l, newdata = new.X.test)
table(y.test, svm.p.test_preds)
table(y.test, svm.l.test_preds)
### In order to generate probabilities we have to specify probability = TRUE
### while fitting/training the model
svm.p.probs = svm(new.X.train, y.train, kernel = 'polynomial', probability = TRUE)
svm.l.probs = svm(new.X.train, y.train, kernel = 'linear', probability = TRUE)
### Now testing on test data
svm.p.probs_test = predict(svm.p.probs, newdata = new.X.test, probability = TRUE)
svm.l.probs_test = predict(svm.l.probs, newdata = new.X.test, probability = TRUE)
library(ROC)
library(roc)
library(proc)
library(pROC)
?roc
### Evaluating the auc value
roc(svm.p.probs_test, y.test)$auc
?ConfusionMatrix
install.packages("caret")
library(caret)
### Evaluating the auc value
confusionMatrix(data = svm.p.probs_test, reference = y.test, positive = TRUE)
### Evaluating the auc value
confusionMatrix(data = svm.p.probs_test, reference = y.test, positive = 'Yes')
confusionMatrix(data = svm.l.probs_test, reference = y.test, positive = 'Yes')
### Computing the AUC value (we need to extract P(Y=1|X))
prob.lin = attributes(svm.l.probs_test)$probabilities[,2]
prob.pol = attributes(svm.p.probs_test)$probabilities[,2]
roc(response=y.test, predictor=p.lin)$auc
roc(response=y.test, predictor=prob.lin)$auc
roc(response=y.test, predictor=prob.pol)$auc
attributes(svm.l.probs_test)
attributes(svm.l.probs_test)$probabilities[,2]
plot(roc(response=y.test, predictor=prob.lin))
####### Exercise - 1 #######
rm(list=ls())
data = iris
View(data)
n = nrow(data)
idx = sample(1:n, 100, replace = TRUE)
X = data
y = data$Species
X$Species = NULL
X.train = X[idx, ]
X.test = X[-idx, ]
y.train = y[idx]
y.test = y[-idx]
View(X.test)
rm(list=ls())
set.seed(4061)
data = iris
n = nrow(data)
idx = sample(1:n, 100)
X = data
y = data$Species
X$Species = NULL
X.train = X[idx, ]
X.test = X[-idx, ]
y.train = y[idx]
y.test = y[-idx]
svm.p = svm(X.train, y.train, kernel = 'polynomial')
svm.l = svm(X.train, y.train, kernel = 'linear')
svm.r = svm(X.train, y.train, kernel = 'radial')
summary(svm.p)
summary(svm.l)
summary(svm.r)
svm.p.preds = predict(svm.p, newdata = X.test)
svm.l.preds = predict(svm.l, newdata = X.test)
svm.r.preds = predict(svm.r, newdata = X.test)
table(svm.p.preds, y.test)
table(svm.l.preds, y.test)
table(svm.r.preds, y.test)
confusionMatrix(svm.p.preds, y.test)
confusionMatrix(svm.l.preds, y.test)
confusionMatrix(svm.r.preds, y.test)
set.seed(4061)
svm.tune = e1071::tune(svm, train.x=x.train, train.y=y.train,
kernel='radial',
ranges=list(cost=10^(-2:2),
gamma=c(0.5,1,1.5,2)))
svm.tune = e1071::tune(svm, train.x=X.train, train.y=y.train,
kernel='radial',
ranges=list(cost=10^(-2:2),
gamma=c(0.5,1,1.5,2)))
print(svm.tune)
names(svm.tune)
bp = svm.tune$best.parameters
bp
new.svm = svm(X.train, y.train, kernel = 'radial', cost = bp$cost, gamma =  bp$gamma)
new.svm.preds = predict(new.svm, newdata = X.test)
confusionMatrix(new.svm.preds, y.test)
set.seed(4061)
rm(list=ls())
data = Hitters
View(data)
rm(list=ls())
set.seed(4061)
data = Hitters
data = na.omit(data)
n = nrow(dat)
n = nrow(data)
data$Salary = as.factor(ifelse(data$Salary>median(data$Salary),
"High","Low"))
View(data)
X = data
y = data$Salary
X$Salary = NULL
idx = sample(1:n, size = 0.7*n)
X.train = X[idx, ]
X.test = X[-idx, ]
y.train = y[idx]
y.test = y[-idx]
## Since the data has many categorical data, performing one hot
## encoding
new_data = model.matrix(y~.+0, data = X)
View(new_data)
new.X.train = new_data[idx, ]
new.X.test = new_data[-idx, ]
y.train = as.factor(y.train)
y.test = as.factor(y.test)
svm.l = svm(new.X.train, y.train, kernel = 'linear')
svm.l.preds = predict(svm.l, newdata = new.X.test)
confusionMatrix(svm.l.preds, y.test)
svm.r = svm(new.X.train, y.train, kernel = 'radial')
svm.r.preds = predict(svm.r, newdata = new.X.test)
confusionMatrix(svm.r.preds, y.test)
?svm
rm(list=ls())
set.seed(4061)
data = iris
View(data)
data$Species = NULL
n = nrow(data)
X = data
y = data$Sepal.Length
X$Sepal.Length = NULL
idx = sample(1:n, 100)
X.train = X[idx, ]
X.test = X[-idx, ]
y.train = y[idx]
y.test = y[-idx]
train_control = trainControl(method = "cv", number = 10)
View(train_control)
?train
model = train(Cost~., data = train, method = "svmRadial", trControl = train_control)
train.data = data.frame(X.train, y.train)
model = train(Sepal.Length~., data = train.data, method = "svmRadial", trControl = train_control)
model = train(y.train~., data = train.data, method = "svmRadial", trControl = train_control)
model = train(y.train~., data = train.data, method = "svmRadial", trControl = train_control)
model
svm.p = predict(model, newdata=X.test)
mean( (y.test-svm.p)^2 )
rm(list=ls())
setwd("D:/UCC/Academic Work/Sem-2/ST6033 - GLM/Lab/ST6033---GLM/Lab-8")
df = read.table("claims.txt")
attach(df)
View(df)
View(df)
df = read.table("claims.txt", header = FALSE)
View(df)
attach(df)
rm(list=ls())
setwd("D:/UCC/Academic Work/Sem-2/ST6033 - GLM/Lab/ST6033---GLM/Lab-8")
df = read.table("claims.txt", header = FALSE)
View(df)
df = read.table("claims.txt", header = TRUE)
View(df)
attach(df)
nrow(df)
plot(df$Claims)
table(df$Claims)
barplot(df$Claims)
barplot(df$Claims, horiz = TRUE)
barplot(df$Claims)
poi_model = glm(Claims ~., data = df, family = 'poisson')
summary(poi_model)
poi_model = glm(Claims ~ as.factor(Gender) + Age + Mileage + as.factor(Province), data = df, family = 'poisson')
summary(poi_model)
pchisq(deviance(poi_model), df.residual(poi_model), lower=FALSE)
leverage = hatvalues(poi_model)
leverage[1]
leverage
deviance(poi_model)
pear_res = residuals(poi_model, 'pearson')/sqrt(1-leverage)
pear_dev = residuals(poi_model)/sqrt(1-leverage)
cooks.distance(poi_model)
cooks.distance(poi_model)[1]
pear_dev[1]
plot(pear_res, pear_dev)
abline(0,1)
unique(df$Claims)
plot(pear_dev)
lp = poi_model$linear.predictors
plot(pear_dev, lp)
plot(lp, pear_dev)
plot(pear_dev)
pear_dev[which(pear_dev > 2)]
length(pear_dev[which(pear_dev > 2)])
plot(lp, pear_dev)
length(pear_dev[which(pear_dev > 3)])
pear_dev[which(pear_dev > 3)]
(2*7)/900
plot(leverage)
lev_thresh = (2*7)/900 ## 2p/n => Leverage value => 0.0156
length(leverage[which(leverage > lev_thresh)])
which.max(leverage)
df[465]
df[465,]
mean(Mileage)
mean(Age)
table
table(Province)
df[465,]
mean(Mileage)
cooks_dist = cooks.distance(poi_model)
plot(cooks_dist)
which.max(cooks_dist)
cooks_dist[which(cooks_dist > 0.03)]
cooks_dist[which(cooks_dist > 0.02)]
which.max(leverage)
leverage[which(leverage > lev_thresh)]
rm(list=ls())
Severe <- c(1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2)
Info <- c(1,1,1,1,2,2,2,2,1,1,1,1,2,2,2,2)
Age <- c(1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4)
n <- c(18,23,22,17,19,35,30,22,24,37,29,24,28,42,37,30)
CB <- c(11,14,11,5,4,15,8,8,10,13,8,6,11,14,15,9)
df = data.frame(Severe, Info, Age, n, CB)
nrow(df)
log_model = glm(cbind(CB, n-CB) ~ as.factor(Severe) + as.factor(Info)
+ as.factor(Age), data = df, family = 'binomial')
summary(log_model)
leverage = hatvalues(log_model)
leverage[1]
cooks_dist = cooks.distance(log_model)
cooks_dist[1]
pear_res = residuals(log_model, 'pearson')/(1-leverage)
pear_res[1]
pear_res = residuals(log_model, 'pearson')/sqrt(1-leverage)
pear_res[1] ## => 1.669829
residuals.glm(log_model, type = 'pearson')[1]
dev_res = residuals(log_model)/sqrt(1-leverage)
dev_res[1]
plot(dev_res, pear_res)
plot(pear_res, dev_res)
plot(dev_res)
plot(pear_res)
pear_res[which(pear_res > -2)]
pear_res[which(pear_res < -2)]
dev_res[which(dev_res < -2)]
lev_thresh = (2*6)/16 ## 2p/n
plot(leverage)
plot(leverage, ylim = c(0.2, 0.8))
abline(h=0.75)
plot(cooks_dist)
which.max(cooks_dist)
