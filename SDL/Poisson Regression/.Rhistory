x = Smarket[,-9]
y = as.factor(Smarket$Direction)
train = sample(1:nrow(Smarket),1000)
data_rf = data.frame(x, y)
### Splitting data into train and test sets ###
train_data = data_rf[train,]
test_data = data_rf[-train,]
### Fitting Random Forest to the training data ###
rf.out = randomForest(y~., train_data)
rf.yhat.train = predict(rf.out, train_data, type="class")
tb.rf.train = table(rf.yhat.train, train_data$y)
tb.rf.train
### Performing predictions on the test data 3(b)###
rf.yhat.test = predict(rf.out, test_data, type = "class")
tb.rf.test = table(rf.yhat.test, test_data$y)
tb.rf.test
### Plot ROC Curve for the test data ###
rf.test.probs = predict(rf.out, test_data, type="prob")
roc.rf = roc(response=(test_data$y), predictor=rf.test.probs[,1])
roc.rf$auc
plot(roc.rf)
### Preparing data for kNN model 3(c)###
x.train = x[train,]
x.test = x[-train,]
y.train = y[train]
y.test = y[-train]
### fitting the data to kNN ###
K = 2
ko = knn(x.train, x.test, y.train, K)
knn.pred = as.numeric(ko == "Up")
tb.knn = table(ko, y.test)
tb.knn
### ROC curve for kNN ###
knn.probs = attributes(knn(x.train, x.test, y.train, K, prob=TRUE))$prob
new.knn.probs = 1 - knn.probs
knn.probs.final = ifelse(knn.pred == 1, knn.probs, new.knn.probs)
roc.knn = roc(y.test ,knn.probs.final)$auc
roc.knn$auc
rm(list=ls())
rm(list=ls())
library(ISLR)
library(class)
library(pROC)
library(randomForest)
set.seed(4061)
### loading the data 3(a)###
x = Smarket[,-9]
y = as.factor(Smarket$Direction)
train = sample(1:nrow(Smarket),1000)
data_rf = data.frame(x, y)
### Splitting data into train and test sets ###
train_data = data_rf[train,]
test_data = data_rf[-train,]
### Fitting Random Forest to the training data ###
rf.out = randomForest(y~., train_data)
rf.yhat.train = predict(rf.out, train_data, type="class")
tb.rf.train = table(rf.yhat.train, train_data$y)
tb.rf.train
### Performing predictions on the test data 3(b)###
rf.yhat.test = predict(rf.out, test_data, type = "class")
tb.rf.test = table(rf.yhat.test, test_data$y)
tb.rf.test
### Plot ROC Curve for the test data ###
rf.test.probs = predict(rf.out, test_data, type="prob")
roc.rf = roc(response=(test_data$y), predictor=rf.test.probs[,1])
roc.rf$auc
plot(roc.rf)
### Preparing data for kNN model 3(c)###
x.train = x[train,]
x.test = x[-train,]
y.train = y[train]
y.test = y[-train]
### fitting the data to kNN ###
K = 2
ko = knn(x.train, x.test, y.train, K)
knn.pred = as.numeric(ko == "Up")
tb.knn = table(ko, y.test)
tb.knn
### ROC curve for kNN ###
knn.probs = attributes(knn(x.train, x.test, y.train, K, prob=TRUE))$prob
new.knn.probs = 1 - knn.probs
knn.probs.final = ifelse(knn.pred == 1, knn.probs, new.knn.probs)
roc.knn = roc(y.test ,knn.probs.final)
roc.knn$auc
plot(roc.knn, add = TRUE, col = "orange")
legend("bottomright",legend = c("Random Forest", "KNN"), col = c("black", "orange"), lwd = 5)
### KNN classification for different k values 3(d)###
set.seed(4061)
M = 1000
train = sample(1:nrow(Smarket), M)
x.train = x[train,]
x.test = x[-train,]
y.train = y[train]
y.test = y[-train]
Kmax = 10
acc = numeric(Kmax)
for(k in 1:Kmax){
ko = knn(x.train, x.test, y.train, k)
tb = table(ko, y.test)
acc[k] = sum(diag(tb)) / sum(tb)
}
misclass.err = 1 - acc
plot(misclass.err, pch=20, t='b', main = "Plot of K vs Misclassification Error Rate",
ylab = "Misclassification Error Rate", xlab="K")
?roc
rm(list=ls())
library(ISLR)
df = Carseats
View(df)
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
CS = data.frame(df, High)
View(CS)
set.seed(4061)
?sample
n = length(df)
idx = sample(1:n, 350)
# simulate data:
set.seed(1)
n = 100
mark = rnorm(n, m=50, sd=10)
choc = rnorm(n, m=60, sd=5)
summary(mark)
summary(choc)
int = 10
a = 2
b = 4
# split the data into train+test (50%-50%):
x = data.frame(mark,choc)
i.train = sample(1:n, 50)
rm(list=ls())
library(ISLR)
df = Carseats
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
CS = data.frame(df, High)
set.seed(4061)
n = length(df)
idx = sample(1:n, 350)
idx = sample(1:n, 350, replace = TRUE)
X.train = CS[idx,]
X.test = CS[-idx,]
y.train = CS[idx]
y.test = CS[-idx]
View(y.train)
rm(list=ls())
library(ISLR)
df = Carseats
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
CS = data.frame(df, High)
set.seed(4061)
n = length(df)
idx = sample(1:n, 350)
rm(list=ls())
library(ISLR)
df = Carseats
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
CS = data.frame(df, High)
set.seed(4061)
idx = sample(1:length(df), 350)
X.train = CS[idx,]
X.test = CS[-idx,]
y.train = CS[idx]
y.test = CS[-idx]
n = nrow(df)
idx = sample(1:n , 350)
X.train = CS[idx,]
X.test = CS[-idx,]
y.train = CS[idx]
y.train = CS[idx]
y.test = CS[-idx]
n = nrow(df)
idx = sample(1:n , 350)
X.train <- CS[idx, ]
X.test <- CS[-idx, ]
y.train <- CS$High[idx]
y.test <- CS$High[-idx]
install.packages("e1071")
install.packages("e1071")
#install.packages("e1071")
library(e1071)
svmo = svm(X.train, y.train, kernel='polynomial')
?svm
rm(list=ls())
rm(list=ls())
library(ISLR)
library(e1071)
df = Carseats
### Converting into binary classification problem
High = ifelse((df$Sales<=8),'No', 'Yes')
View(df)
CS = data.frame(df, High)
View(CS)
CS$Sales = NULL
X = CS
y = CS$High
X$High = NULL
set.seed(4061)
n = nrow(X)
idx = sample(1:n , 350)
X.train = X[idx, ]
X.test = X[-idx]
y.train = y[idx, ]
y.train = y[idx]
y.test = y[-idx]
X.test = X[-idx, ]
### Fitting SVM model (from e1071 package)
svmo = svm(X.train, y.train, kernel='polynomial')
View(X.train)
### Error occuring here is because the covariates cannot be supported
### in factor category, so perform one hot encoding to convert them.
new_data = model.matrix(y~.+0, data = X)
View(new_data)
new.X.train = new_data[idx, ]
new.X.test = new_data[-idx, ]
y.train = as.factor(y.train)
### Now fitting the SVM model
svm.p = svm(new.X.train, y.train, kernel = 'polynomial')
summary(svm.p)
svm.l = svm(new.X.train, y.train, kernel = 'linear')
summary(svm.l)
plot(svm.p)
svm.p.train_preds = fitted(svm.p)
svm.l.train_preds = fitted(svm.l)
table(y.train, svm.p.train_preds)
table(y.train, svm.l.train_preds)
### Evaluating model on test data
svm.p.test_preds = predict(svm.p, newdata = new.X.test)
y.test = as.factor(y.test)
svm.l.test_preds = predict(svm.l, newdata = new.X.test)
table(y.test, svm.p.test_preds)
table(y.test, svm.l.test_preds)
### In order to generate probabilities we have to specify probability = TRUE
### while fitting/training the model
svm.p.probs = svm(new.X.train, y.train, kernel = 'polynomial', probability = TRUE)
svm.l.probs = svm(new.X.train, y.train, kernel = 'linear', probability = TRUE)
### Now testing on test data
svm.p.probs_test = predict(svm.p.probs, newdata = new.X.test, probability = TRUE)
svm.l.probs_test = predict(svm.l.probs, newdata = new.X.test, probability = TRUE)
library(ROC)
library(roc)
library(proc)
library(pROC)
?roc
### Evaluating the auc value
roc(svm.p.probs_test, y.test)$auc
?ConfusionMatrix
install.packages("caret")
library(caret)
### Evaluating the auc value
confusionMatrix(data = svm.p.probs_test, reference = y.test, positive = TRUE)
### Evaluating the auc value
confusionMatrix(data = svm.p.probs_test, reference = y.test, positive = 'Yes')
confusionMatrix(data = svm.l.probs_test, reference = y.test, positive = 'Yes')
### Computing the AUC value (we need to extract P(Y=1|X))
prob.lin = attributes(svm.l.probs_test)$probabilities[,2]
prob.pol = attributes(svm.p.probs_test)$probabilities[,2]
roc(response=y.test, predictor=p.lin)$auc
roc(response=y.test, predictor=prob.lin)$auc
roc(response=y.test, predictor=prob.pol)$auc
attributes(svm.l.probs_test)
attributes(svm.l.probs_test)$probabilities[,2]
plot(roc(response=y.test, predictor=prob.lin))
####### Exercise - 1 #######
rm(list=ls())
data = iris
View(data)
n = nrow(data)
idx = sample(1:n, 100, replace = TRUE)
X = data
y = data$Species
X$Species = NULL
X.train = X[idx, ]
X.test = X[-idx, ]
y.train = y[idx]
y.test = y[-idx]
View(X.test)
rm(list=ls())
set.seed(4061)
data = iris
n = nrow(data)
idx = sample(1:n, 100)
X = data
y = data$Species
X$Species = NULL
X.train = X[idx, ]
X.test = X[-idx, ]
y.train = y[idx]
y.test = y[-idx]
svm.p = svm(X.train, y.train, kernel = 'polynomial')
svm.l = svm(X.train, y.train, kernel = 'linear')
svm.r = svm(X.train, y.train, kernel = 'radial')
summary(svm.p)
summary(svm.l)
summary(svm.r)
svm.p.preds = predict(svm.p, newdata = X.test)
svm.l.preds = predict(svm.l, newdata = X.test)
svm.r.preds = predict(svm.r, newdata = X.test)
table(svm.p.preds, y.test)
table(svm.l.preds, y.test)
table(svm.r.preds, y.test)
confusionMatrix(svm.p.preds, y.test)
confusionMatrix(svm.l.preds, y.test)
confusionMatrix(svm.r.preds, y.test)
set.seed(4061)
svm.tune = e1071::tune(svm, train.x=x.train, train.y=y.train,
kernel='radial',
ranges=list(cost=10^(-2:2),
gamma=c(0.5,1,1.5,2)))
svm.tune = e1071::tune(svm, train.x=X.train, train.y=y.train,
kernel='radial',
ranges=list(cost=10^(-2:2),
gamma=c(0.5,1,1.5,2)))
print(svm.tune)
names(svm.tune)
bp = svm.tune$best.parameters
bp
new.svm = svm(X.train, y.train, kernel = 'radial', cost = bp$cost, gamma =  bp$gamma)
new.svm.preds = predict(new.svm, newdata = X.test)
confusionMatrix(new.svm.preds, y.test)
set.seed(4061)
rm(list=ls())
data = Hitters
View(data)
rm(list=ls())
set.seed(4061)
data = Hitters
data = na.omit(data)
n = nrow(dat)
n = nrow(data)
data$Salary = as.factor(ifelse(data$Salary>median(data$Salary),
"High","Low"))
View(data)
X = data
y = data$Salary
X$Salary = NULL
idx = sample(1:n, size = 0.7*n)
X.train = X[idx, ]
X.test = X[-idx, ]
y.train = y[idx]
y.test = y[-idx]
## Since the data has many categorical data, performing one hot
## encoding
new_data = model.matrix(y~.+0, data = X)
View(new_data)
new.X.train = new_data[idx, ]
new.X.test = new_data[-idx, ]
y.train = as.factor(y.train)
y.test = as.factor(y.test)
svm.l = svm(new.X.train, y.train, kernel = 'linear')
svm.l.preds = predict(svm.l, newdata = new.X.test)
confusionMatrix(svm.l.preds, y.test)
svm.r = svm(new.X.train, y.train, kernel = 'radial')
svm.r.preds = predict(svm.r, newdata = new.X.test)
confusionMatrix(svm.r.preds, y.test)
?svm
rm(list=ls())
set.seed(4061)
data = iris
View(data)
data$Species = NULL
n = nrow(data)
X = data
y = data$Sepal.Length
X$Sepal.Length = NULL
idx = sample(1:n, 100)
X.train = X[idx, ]
X.test = X[-idx, ]
y.train = y[idx]
y.test = y[-idx]
train_control = trainControl(method = "cv", number = 10)
View(train_control)
?train
model = train(Cost~., data = train, method = "svmRadial", trControl = train_control)
train.data = data.frame(X.train, y.train)
model = train(Sepal.Length~., data = train.data, method = "svmRadial", trControl = train_control)
model = train(y.train~., data = train.data, method = "svmRadial", trControl = train_control)
model = train(y.train~., data = train.data, method = "svmRadial", trControl = train_control)
model
svm.p = predict(model, newdata=X.test)
mean( (y.test-svm.p)^2 )
rm(list = ls())
install.packages("foreign")
#install.packages("foreign")
setwd("D:/UCC/Academic Work/Sem-2/ST6033 - GLM/Lab/ST6033---GLM/Lab-6")
df = read.dta("mroz.dta")
#install.packages("foreign")
library(foreign)
setwd("D:/UCC/Academic Work/Sem-2/ST6033 - GLM/Lab/ST6033---GLM/Lab-6")
df = read.dta("mroz.dta")
attach(df)
names(df)
View(df)
log_mod = glm(inlf ~ age + kidslt6 + kidsge6, data = df, family = 'binomial')
summary(log_mod)
leverage = lm.influence(log_mod)
lev_hat_vals = leverage$hat
## Alternative method ##
leverage_hat_vals = hatvalues(log_mod)
pear_res = residuals(log_mod, 'pearson')/sqrt(1-lev_hat_vals)
dev_res = residuals(log_mod, 'deviance')/sqrt(1-lev_hat_vals)
cooks_dist = (pear_res*pear_res*lev_hat_vals)/(4*(1-lev_hat_vals))
## Alternative method for cooks distance ##
alt_cooks_dist = cooks.distance(log_mod)
log_linear_preds = log_mod$linear.predictors
plot(log_linear_preds, pear_res)
pchisq(deviance(log_mod), df.residual(log_mod), lower=FALSE)
plot(pear_res)
identify(pear_res, n=2)
identify(pear_res, n=2)
pear_res[which(pear_res > 2) & which(pear_res < -2)]
pear_res[which(pear_res > 2)]
pear_res[which(pear_res < -2)]
plot(lev_hat_vals)
## leverage_threshold = 2*p/n ##
lev_thresh = (2*4)/753
abline(h=lev_thresh)
length(lev_hat_vals[which(lev_hat_vals>lev_thresh)])
which.max(lev_thresh)
which.max(lev_hat_vals)
identify(lev_hat_vals, n=2)
?identify
identify(lev_hat_vals, n=2)
text(lev_hat_vals, labels = seq_along(lev_hat_vals), pos = 1)
plot(cooks_dist)
text(cooks_dist, labels = seq_along(cooks_dist), pos = 1)
df[c(74, 400)]
df[74,]
df[400,]
df[74, c(1,3,4,5)]
df[400,c(1,3,4,5)]
mean(age)
df[c(53, 720), c(1,3,4,5)]
summary(kidsge6)
rm(list=ls())
Age = c(1, 2, 3, 4)
Using = c(72, 105, 237, 93)
n = c(397, 404, 612, 194)
cbind(Using, n-Using)
df = data.frame(Age, Using, n)
log_model = glm(cbind(Using, n-Using) ~ as.factor(Age), data = df,
family = 'binomial')
summary(log_model)
exp(log_model$coefficients)
odds_rat = 2.852778/0.2215385
rm(list=ls())
Age = c(1, 2, 3, 4)
rm(list=ls())
Age = c(1, 2, 3, 4)
Using = c(72, 105, 237, 93)
n = c(397, 404, 612, 194)
df = data.frame(Age, Using, n)
log_model = glm(cbind(Using, n-Using) ~ as.factor(Age), data = df,
family = 'binomial')
summary(log_model)
-1.5072 + 0.4607(1)
-1.5072 + 0.4607*1
-1.5072 + 1.0483*1
-1.5072 + 1.4286*1
exp(-1.5072)
exp(-1.0465)
exp(-0.4589)
exp(-0.0786)
odds_ratio = 0.6319784/0.2215294
odds_ratio
exp(1.0483)
AgeC<-c(20, 27.5, 35,45)
log_model_2 = glm(cbind(Using, n-Using) ~ AgeC, family = 'binomial')
summary(log_model_2)
exp(0.060671)
confint(log_model_2)
exp(confint(log_model_2))[2]
exp(confint(log_model_2))
rm(list=ls())
yes = c(20, 15, 15)
total = c(200, 100, 50)
log_mod = glm(cbind(yes, total-yes) ~ as.factor(risk), family = 'binomial')
risk = c("L", "M", "H")
yes = c(20, 15, 15)
total = c(200, 100, 50)
log_mod = glm(cbind(yes, total-yes) ~ as.factor(risk), family = 'binomial')
summary(log_mod)
p1 = 20/200
p2 = 15/100
p3 = 15/50 ##
p4 = 50/350
## for reduced model
reduced_mod = glm(cbind(yes, total-yes) ~ 1, family = 'binomial')
summary(reduced_mod)
anova(log_mod, reduced_mod)
pchisq(deviance(reduced_mod), df.residual(reduced_mod))
pchisq(deviance(reduced_mod), df.residual(reduced_mod), lower=FALSE)
rm(list=ls())
library("daewr")
setwd("D:/UCC/Academic Work/Sem-2/ST6033 - GLM/Lab/ST6033---GLM/SDL/Poisson Regression")
gala = read.table("gala.txt")
gala <- gala[,-2]
modp <- glm(Species ~ .,family=poisson,gala)
plot(modp)
plot(residuals(modp) ~ predict(modp,type="response"),xlab=expression(hat(mu)),ylab="Deviance residuals")
plot(residuals(modp) ~ predict(modp,type="link"),xlab=expression(hat(eta)),ylab="Deviance residuals")
plot(residuals(modp,type="response") ~ predict(modp,type="link"),xlab=expression(hat(eta)),ylab="Response residuals")
plot(Species ~ Area, gala)
mu <- predict(modp,type="response")
z <- predict(modp)+(gala$Species-mu)/mu
plot(z ~ log(Area), gala,ylab="Linearized Response")
?predict
plot(z ~ log(Area), gala,ylab="Linearized Response")
abline(0,1)
plot(z ~ log(Area), gala,ylab="Linearized Response")
